<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/laboratorio05/styles.css">
  <title>Gurú Hi Experience</title>
</head>
<body>
  <header>
    <nav>
      <div class="logo">
        <img src="/laboratorio05/images/guru-icon-1.svg" alt="logo guru-hi-experience" width="100">
      </div>
      <ul>
        <li><a href="#articulo1">Artículo 1</a></li>
        <li><a href="#articulo2">Artículo 2</a></li>
        <li><a href="#articulo3">Artículo 3</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <h1>Artículos</h1>
    <ol>
      <li><h2 id="articulo1">Residual Off-Policy RL for Finetuning Behavior Cloning Policies</h2></li>
      <ul>
        <li><h3>Introduction</h3></li>
        <p>Enabling robots to learn and improve directly in their deployment environments remains a fundamental challenge in robotics. Recently, significant progress has been made in training visuomotor control policies in the real world with behavior cloning (BC) from human demonstrations [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, this success requires significant infrastructure, as well as numerous hours of manual and cumbersome data collection. Even if unlimited data could be collected for every task, not only is human teleoperator performance generally suboptimal, but there is also emerging evidence that policy performance saturates with increasing demonstrations [10, 6, 11, 12, 13].</p>
        <br>
        <p>Reinforcement learning (RL) offers a complementary paradigm: agents learn autonomously through trial and error in the environment. Deep RL has shown great success in various domains [14, 15, 16, 17, 18, 19, 20, 21], including in-hand manipulation [22, 23] and locomotion [24, 25, 26, 27]. However, strong RL performance generally requires large amounts of data from online interactions, so its application has been mainly in simulation [28, 29] since real-world data are expensive and potentially unsafe to gather in large amounts.</p>
        <br>
        <p>
        A natural direction to improve BC policies is to leverage online RL [12, 30, 31, 32], combining the strengths of each: BC policies provide a strong prior that can regularize exploration in the RL process, while online RL enhances policy performance by learning from interactions with the environment. However, modern BC architectures are typically deep models with tens of millions to billions of parameters that utilize action chunking or diffusion-based approaches, which can make it challenging to apply RL methods directly to optimize the policy. A simple yet powerful recipe that avoids several of the above issues is residual RL [33, 34, 35, 36, 12, 32, 31], where RL is applied not to learn a full policy, but only to learn corrective terms on top of a fixed base controller. Previous work has demonstrated that residual RL can indeed enhance the reliability of a pre-trained policy. Still, it has so far been limited to learning in simulation [12, 32, 31] or demonstrating results in simple or constrained settings [33, 34, 35, 36]; Applications to high-DoF systems learning directly in the real world are still lacking.</p>
      </ul>
      <img src="https://arxiv.org/html/2509.19301v1/figs/pipeline.png" alt="articulo1" width="500">
      <li><h2 id="articulo2">CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</h2></li>
      <ul>
        <li><h3>Introduction</h3></li>
        <p>Conditional generative models enable to draw samples conditioned on an external variable—for example, a class label, a text caption, or a semantic mask. They are a key technology that has advanced significantly in the last decade, from variational auto-encoders (VAEs) (Kingma and Welling, 2014) and generative adversarial nets (Goodfellow et al., 2014) to diffusion (Ho et al., 2020; Song et al., 2021a, b; Dhariwal and Nichol, 2021; Peebles and Xie, 2023; Rombach et al., 2022; Nichol and Dhariwal, 2021) and flow-matching (Ma et al., 2024; Liu et al., 2023; Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Albergo et al., 2023).</p>
        <br>
        <p>State-of-the-art diffusion and flow-matching frameworks accomplish conditional generation by using a trained deep net to trace a probability path that progressively transforms samples from a simple source distribution into the rich, condition-dependent target distribution. A popular choice for the source distribution is a single, condition-agnostic standard Gaussian, independent of the condition. Consequently, the conditioning signal enters only through the network itself: in flow matching, for instance, the model predicts a velocity field where the condition is commonly incorporated via embeddings or adaptive normalization layers. Although this strategy has enabled impressive results, this design forces the network to shoulder two tasks simultaneously—(i) transporting probability mass to the correct region of the data manifold, and (ii) encoding the semantic meaning of the condition. Because different conditions often occupy distant parts of that manifold, the dual burden stretches the learned trajectory, slows convergence, and can impair sample quality and diversity.</p>
        <br>
        <p>In this paper, we alleviate this burden by letting the distributions themselves depend on the condition. First, we endow the source distribution with condition-awareness through a lightweight map, i.e., the source distribution map. Hence, every condition has its own source distribution. The same idea can be mirrored to the target distribution using another lightweight map, i.e., the target distribution map, that we require to be approximately invertible to keep sampling tractable. The resulting push-forward chain is illustrated in Figure 1. Intuitively, the target distribution map and its approximated inverse correspond to the encoder and decoder, commonly used in latent diffusion pipelines (Rombach et al., 2022). Differently, here, the target distribution map is explicitly conditional, aligning our formulation with recent latent-space conditioning approaches that embed semantic information directly into the latent representation (Wu et al., 2024; Leng et al., 2025; Yao et al., 2025).</p>
      </ul>
      <img src="https://arxiv.org/html/2509.19300v1/x1.png" alt="articulo2" width="400">
      <li><h2 id="articulo3">VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</h2></li>
      <ul>
        <li><h3>Introduction</h3></li>
        <p>3D reconstruction is a cornerstone of modern robotics, empowering autonomous systems with the critical ability to perceive, map, and comprehend their physical environment, which is fundamental for advanced navigation, object manipulation, and intelligent interaction. Traditional optimization based approaches, including Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting (3DGS) [2], obtain high fidelity results by iteratively enforcing photometric or geometric consistency. These methods achieve excellent accuracy but are computationally intensive and slow to run at inference time. By contrast, feed-forward approaches [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] trade per instance optimization for fast learned inference. A single forward pass predicts scene geometry or a 3D representation directly from input images. This speed and simplicity make feed-forward systems attractive for real time applications, large scale datasets, and downstream tasks that require many reconstructions.</p>
      </ul>
      <img src="https://arxiv.org/html/2509.19297v1/x1.png" alt="articulo2" width="600">

    </ol>
  </main>
  <footer>
      <p>&copy; 2025 Laboratorio de Introducción al Desarrollo Web</p>
      <p>Contacto: lgarciada@unsa.edu.pe</p>
  </footer>
</body>
</html>
